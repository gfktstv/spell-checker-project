{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Первичная обработка текста: токенизация\n",
        "\n",
        "Прежде, чем находить и исправлять опечатки и ошибки, необходимо разбить текст на слова, знаки препинания и прочие языковые единицы. Этот процесс сегментации текста называется токенизация, а полученные единицы (сегменты) это токены.\n",
        "\n",
        "С первого взгляда может показаться, что достаточно просто разбить текст по пробелам и мы получим все токены. Но останутся знаки препинания. Когда и они будут учтены — окажется, например, что точка служит не только как конец предложения, но и для написания дат (21.06.2024). Кроме того, есть ещё имена собственные (New York), аббревиатуры (НГУ, ЛШ), составные слова (кресло-кровать), неразрывные неизменяемые словосочетания (и_так_далее; таким_образом), интернет-адреса (http://yandex.ru) и т. д. Короче говоря, задача на самом деле не тривиальная, но мы постараемся её решить."
      ],
      "metadata": {
        "id": "ILAZtItj8DPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Способы токенизации\n",
        "Мы рассмотрим 3 способа: составим простой алгоритм на python, воспользуемся библиотекой и попробуем использовать регулярные выражения."
      ],
      "metadata": {
        "id": "8LTJWkHv9pkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Простой алгоритм на python\n",
        "Давайте начнём с простого, разобём текст по пробелам. Для этого на python существует метод split(), который разбивает строку (str) по выбранным символам. Рассмотрим пример:"
      ],
      "metadata": {
        "id": "GM8sSSfAI0rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Все люди как пипл, а мы ФИПЛ'\n",
        "sentence.split()\n"
      ],
      "metadata": {
        "id": "Zox41XaB8B4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Несмотря на то, что аргумент метода split() пустой, по умолчанию он разбивает строку на пробелы. Если сделать то же самое, но указать в качестве аргумента пробел, мы получим тот же результат:"
      ],
      "metadata": {
        "id": "iVPDYO3P-e6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.split(' ')  # В кавычках пробел"
      ],
      "metadata": {
        "id": "G-0wmG75-lO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод split() возвращает список (list) с элементами строки. Несложно заметить, что слово \"пипл,\" содержит запятую. Чтобы избавиться от неё, необходимо применить split(',') к каждому элементу списка. Для этого воспользуемся циклом for, который будет \"пробегать\" по каждому элементу списка и что-то с ним делать. Рассмотрим на примере:"
      ],
      "metadata": {
        "id": "TRXjtOPv-rhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for element in sentence.split(' '):  # Для каждого element в списке\n",
        "    print(element)  # Выводим на экран сам element\n"
      ],
      "metadata": {
        "id": "DRPYFEO1CM3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Воспользуйся полученными знаниями и добавь внутрь цикла метод split(), указав в качестве аргумента запятую:"
      ],
      "metadata": {
        "id": "sj-yzoavC3o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for element in sentence.split(' '):\n",
        "    # Напиши внутри print метод split() для element\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "eCd6MDogDB8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Надеюсь, что у тебя всё получилось! Однако, если бы мы учитывали все тонкости токенизации при построении такого алгоритма, то у нас ушло бы на это очень много времени. Поэтому рассмотрим другие решения нашей задачи."
      ],
      "metadata": {
        "id": "Y3FOk8PyDaif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Профессиональное решение\n",
        "Воспользуемся библиотекой spaCy, для этого её нужно импортировать, воспользуемся кодом ниже:"
      ],
      "metadata": {
        "id": "7QsfwKMhSaUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "LbqpCb8ITBG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google colab это популярная платформа для работы с нейросетями, поэтому нам не придётся устанавливать библиотеку. И да, spaCy использует нейросеть для токенизации и другой обработки текста. Чтобы воспользоваться spaCy нужно загрузить малую обученную модель, не будем углубляться в детали и просто воспользуемся кодом ниже:"
      ],
      "metadata": {
        "id": "UuQUJBktTF_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "I36RphIeUaj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь мы просто используем nlp как функцию для обработки строки"
      ],
      "metadata": {
        "id": "iXoxvsE6Udtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description = \"\"\"\n",
        "In computing, a pipeline, also known as a data pipeline,\n",
        "is a set of data processing elements connected in series,\n",
        "where the output of one element is the input of the next one.\n",
        "The elements of a pipeline are often executed in parallel\n",
        "or in time-sliced fashion.\n",
        "— Wikipedia\n",
        "\"\"\"\n",
        "\n",
        "nlp(description)\n"
      ],
      "metadata": {
        "id": "pvBlhx8qUnav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выглядит не так, как раньше, ведь это не привычный нам список (list). Давайте преобразуем его с помощью функции list()"
      ],
      "metadata": {
        "id": "yb03L9p-VJ54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(nlp(description))"
      ],
      "metadata": {
        "id": "g4re9VTsVQwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот это уже похоже на правду! Давайте ближе к делу. Мы хотим получить из текста только слова, которые легко распознать как слова с опечаткой. То есть мы пометим все знаки препинания, имена собственные и так далее, чтобы потом мы могли их отсеять.\n",
        "\n",
        "Для этого мы воспользуемся словарём. Словарь это набор пар ключ-значение. Ключом и значением ключа могут быть любые объекты python: другие словари, списки, множества, целые числа и числа с плавающей запятой, строки, булевые выражения и так далее. Например:"
      ],
      "metadata": {
        "id": "z6UoD_KzVUp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Словарь characters_dict имеет пары ключ-значение,\n",
        "# где ключ это имя персонажа в формате str,\n",
        "# а значение это информация о персонаже в формате dict (словарь)\n",
        "characters_dict = {\n",
        "    'Jesse Pinkman': {\n",
        "        'Actor': 'Aaron Paul',  # Строка - строка\n",
        "        'Age': 26,  # Строка - целое число\n",
        "        'Cash': 700000.64,  # Строка - число с плавающей точкой\n",
        "        'Aliases': ['Pinkman kid']  # Строка - список\n",
        "    },\n",
        "    'Walter White': {\n",
        "        'Actor': 'Bryan Cranston',\n",
        "        'Age': 52,\n",
        "        'Cash': 80000000.43,\n",
        "        'Aliases': ['Mr.White', 'Heisenberg']\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "cksJeQJoFfqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве ключа нашего словаря будет токен, а в качестве значения True или False. Для некоторых слов мы не сможем проверить правописание, поэтому, если слово НЕ подходит для рассмотрения на предмет опечаток, то True, иначе False. Рассмотрим на примере:"
      ],
      "metadata": {
        "id": "jhZG054UYWcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_dict = {\n",
        "    'Walter White': True,  # Имя собственное не рассматриваем\n",
        "    'cat': False,\n",
        "    'NSU': True,  # Аббревиатуры не рассматриваем\n",
        "    'bread': False\n",
        "}\n",
        "\n",
        "tokens_dict\n"
      ],
      "metadata": {
        "id": "QxavmES3YnaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь нам нужно сделать словарь из тех токенов, которые мы получили с помощью spaCy. Здесь важно кое-что прояснить: токены spaCy это не просто текст, но также и другая лингвистическая информация, например часть речи. Рассмотрим на примере из документации spaCy:"
      ],
      "metadata": {
        "id": "BSibz_aajiF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализируем doc c помощью функции nlp()\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos, token.pos_)\n"
      ],
      "metadata": {
        "id": "sASgBON-ZEDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "У токена есть разные атрибуты, то есть переменные, которые принадлежат этому объекту. Например, атрибут `text` это токен в привычном нам формате str. Атрибуты `pos` и `pos_` это часть речи (Part Of Speech, pos) в разной записи — в формате числа и строки.\n",
        "\n",
        "Таких атрибутов у токена довольно много, но нас будут интересовать только текст и часть речи: текст будем использовать в качестве ключа словаря, а значение (True/False) определим на основе части речи. То есть какие-то части речи мы будем рассматривать для исправления опечаток, а какие-то нет. Рассмотрим на примере:"
      ],
      "metadata": {
        "id": "U9Fq2KizkYFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализируем doc c помощью функции nlp()\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "    key = token.text  # Текст токена будет ключом в словаре\n",
        "    value = token.pos_ != 'VERB'  # Если часть речи токена это VERB (глагол),\n",
        "                                  # То тогда значение в словаре False,\n",
        "                                  # иначе True\n",
        "\n",
        "    print(key, value)\n"
      ],
      "metadata": {
        "id": "eF1sPslNkWHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном примере мы используем цикл, в котором пробегаемся по всем токенам текста. В качестве ключа мы используем текст токена:\n",
        "\n",
        "`key = token.text   # Переменная key содержит текст токена`\n",
        "\n",
        "В качестве значения мы хотим получить True/False в зависимости от части речи:\n",
        "\n",
        "`value = token.pos_ != 'VERB'`\n",
        "\n",
        "Символ `!=` проверяет неравенство значений слева и справа от знака. Если часть речи токена это VERB, то мы получим False и переменная value будет содержать False, в ином случае True."
      ],
      "metadata": {
        "id": "tgTjBhxIlv6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот мы и составили пары ключ-значение на основе текста. Теперь осталось отсеять все ненужные нам части речи. Мы уже подготовили список не подходящих частей речи, запусти код ниже, чтобы инициализоровать переменную banned_pos"
      ],
      "metadata": {
        "id": "tTLobNZ61Wpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']"
      ],
      "metadata": {
        "id": "C3w-0LGHluba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы проверить, содержится ли часть речи в списке banned_pos, нужно написать следующее выражение:"
      ],
      "metadata": {
        "id": "WfHJlA81mkTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'VERB' in banned_pos"
      ],
      "metadata": {
        "id": "WS0Xmep115WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь давайте решать нашу задачу. Для начала напиши код, который будет выводить на экран слово и False/True в зависимости от того, находится ли оно в banned_pos:"
      ],
      "metadata": {
        "id": "cuZvsbFY3kgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Jesse, we have to cook'\n",
        "\n",
        "# Инициализируем doc c помощью функции nlp()\n",
        "doc = nlp(text)\n",
        "\n",
        "# Напиши свой код ниже\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8cIDf0X3j3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, теперь сделаем из этого словарь. Просто добавь перед циклом for переменную и инициализируй словарь вот так:\n",
        "\n",
        "`tokens_dict = dict()`\n",
        "\n",
        "Затем вместо того, чтобы напечатать слово и значение True/False, сделаем пару ключ-значение, обратившись к словарю `token_dict`:\n",
        "\n",
        "```\n",
        "for token in doc:\n",
        "  key =  # Здесь текст токена\n",
        "  value =  # Здесь True, если часть речи в banned_pos\n",
        "           # и False, если часть речи НЕ в banned_pos\n",
        "  \n",
        "  tokens_dict[key] = value  # Создаём пару ключ-значение\n",
        "```\n",
        "\n",
        "Напиши код ниже и выведи словарь на экран с помощью print(). Заметь, что в цикле появилась дополнительная проверка с комментарием \"Исключим перенос строки\". Не удаляй её и вставь в последующие циклы."
      ],
      "metadata": {
        "id": "3X0sHLvx50R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Jesse, we have to cook'\n",
        "\n",
        "# Инициализируем части речи, которые мы не будем рассматривать\n",
        "banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']\n",
        "\n",
        "# Инициализируй словарь перед циклом\n",
        "\n",
        "\n",
        "# Инициализируй doc, обработав текст с помощью функции nlp()\n",
        "\n",
        "\n",
        "for token in doc:\n",
        "    key =  # Присвой переменной key значение текста токена\n",
        "    value =   # Присвой переменной value True/False в зависимости от части речи\n",
        "\n",
        "    # Исключим перенос строки, потому что его часть речи неизвестна\n",
        "    # и если мы добавим её в banned_pos, то пропустим некоторые опечатки,\n",
        "    # потому что их часть речи также бывает неизвестна\n",
        "    if key == '\\n':\n",
        "        value = True\n",
        "\n",
        "    # Создай пару ключ-значение\n",
        "\n",
        "\n",
        "print()  # Напиши переменную словаря внутрь функции print\n"
      ],
      "metadata": {
        "id": "8_oK3OaW5xBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, теперь осталось оформить всё это в функции. Функция работает очень просто: мы даём ей какое-то название, определяем, будет ли она что-то принимать в качестве аргумента и затем пишем внутри то, что она будет делать. Вот так:"
      ],
      "metadata": {
        "id": "Yk-oVEMo2IUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tokens(text):  # Def это ключевое слово для функции,\n",
        "                         # print_tokens это название,\n",
        "                         # text это аргумент\n",
        "\n",
        "    print(text.split())\n",
        "\n",
        "text = 'Jesse, we have to cook'  # Напишем какую-нибудь строку\n",
        "print_tokens(text)  # Вызовем функцию и передадим ей эту строку\n"
      ],
      "metadata": {
        "id": "lVgED_LD2O9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь финальное задание. У нас есть функция get_tokens_dictionary, её аргумент это какая-то строка. Нам нужно взять тот код, который на основе строки создаёт словарь и вставить его внутрь функции. И самое главное, создание словаря и его заполнение должно быть внутри функции."
      ],
      "metadata": {
        "id": "vKso4LZh3Rv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens_dict(text):\n",
        "    # Инициализируй части речи, которые мы не будем рассматривать\n",
        "\n",
        "\n",
        "    # Инициализируй словарь\n",
        "\n",
        "\n",
        "    # Инициализируй doc, обработав текст с помощью функции nlp()\n",
        "\n",
        "\n",
        "    # Заполни словарь токенами и True/False c помощью цикла for\n",
        "\n",
        "\n",
        "    return tokens_dict\n"
      ],
      "metadata": {
        "id": "V8OC30ji3Q9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запусти этот код, чтобы проверить, работает ли функция\n",
        "\n",
        "sample_text = '''\n",
        "Levenshtein distnce, also knowm as edit distance, is a metric\n",
        "fo measuring the difference between two sequencess.\n",
        "It represents the minimum numbet of single-character editk\n",
        "(insertions, deletons, or substititions) requirуd to change\n",
        "one word or string ingo the other.\n",
        "'''\n",
        "\n",
        "get_tokens_dict(sample_text)\n"
      ],
      "metadata": {
        "id": "hfmjecu8-EWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Надеюсь, что всё работает! Далее мы будем использовать эту функцию в приложении, поэтому сохрани свой результат (ctrl + s) и скачай notebook:\n",
        "\n",
        "**file --> download --> download .ipynb**"
      ],
      "metadata": {
        "id": "OA8wOSWdPvnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Алтернативный способ: регулярные выражения\n",
        "Задача решена. Но какие есть другие способы токенизировать текст? Одним из неплохих решений можно считать использование регулярных выражений, давайте посмотрим что это такое:"
      ],
      "metadata": {
        "id": "sfWjFTswDsL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # Импортируем библиотеку регулярных выражений\n",
        "\n",
        "sentence = 'Все люди, как пипл, а мы ФИПЛ'\n",
        "print(re.split(' |, ', sentence), 'regex в деле')\n",
        "print(sentence.split(' |, '), 'встроенный split')\n"
      ],
      "metadata": {
        "id": "k0x_9Mt6Ev9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как же это работает? Регулярные выражения позволяют использовать в качестве паттерна различные символы. Посмотрим на примере:\n",
        "\n",
        "`re.split(' |, ', sentence)`\n",
        "\n",
        "re.split использует в качестве аргумента для разбиения строки паттерн в кавычках `' |, '`, в котором указан символ `|`, он же \"или\". Этот символ позволяет выбирать по каким символам разбивать строку: по пробелу (указан слева от `|`) или по запятой и пробелу (указаны справа от `|`). Поэтому в итоге слово \"пипл\" не содержит запятой, ведь после него стояла запятая и пробел, которую учёл regex."
      ],
      "metadata": {
        "id": "u0qUzaLhFm8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Но что, если мы хотим учесть точку и другие знаки препинания? Для этого воспользуемся в паттерне квадратными скобками: они позволяют нам учесть все паттерны, в которых содержится один из элементов, указанных внутри квадратных скобок. Рассмотрим на примере:"
      ],
      "metadata": {
        "id": "SC2ItcWyHMMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Все люди, как пипл, а мы фипл'\n",
        "re.findall('[фп]ипл', sentence)\n"
      ],
      "metadata": {
        "id": "tvJCpTUzHLkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод findall позволяет найти всё то, что указано в паттерне. Как видно, в паттерне мы указали `'[фп]ипл'`, то есть мы ищем либо фипл (ф из квадратных скобок), либо пипл (п из квадратных скобок).\n",
        "\n",
        "Теперь попробуй самостоятельно дополнить паттерн `' |, '` другими знаками препинания, используя квадратные скобки на месте запятой:"
      ],
      "metadata": {
        "id": "aj_sU1SpIZa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Все люди, как пипл, а мы ФИПЛ'\n",
        "re.split(' |, ', sentence)\n"
      ],
      "metadata": {
        "id": "CFoKii_KHx2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь, когда мы знаем что такое регулярные выражение, что такое паттерн и какие символы он может использовать — рассмотрим как с помощью него можно осуществить токенизацию текста:"
      ],
      "metadata": {
        "id": "5PwYQcYfI7g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prayer = \"\"\"\n",
        "Ave, Maria, gratia plena; Dominus tecum; benedicta tu in mulieribus,\n",
        "et benedictus fructus ventris tui, Iesus. Sancta Maria, Mater Dei,\n",
        "ora pro nobis peccatoribus, nunc et in hora mortis nostrae. Amen.\n",
        "\"\"\"\n",
        "\n",
        "re.findall('\\w+', prayer)\n"
      ],
      "metadata": {
        "id": "Y6pLsBpzJGzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью findall мы ищем всё, что подходит под паттерн `\\w+`, который состоит из двух элементов: `\\w` и `+`. `\\w` идентичен `'[a-zA-Z0-9_]'`, а `+` это 1 или более символов. То есть мы ищем 1 или более символов из набора всех латинских букв (строчных и прописных), цифр и нижнего подчёркивания.\n",
        "\n",
        "Проверь это, записав re.findall с использованием идентичного паттерна вместо `\\w`"
      ],
      "metadata": {
        "id": "jhbT6vIMOM1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prayer = \"\"\"\n",
        "Ave, Maria, gratia plena; Dominus tecum; benedicta tu in mulieribus,\n",
        "et benedictus fructus ventris tui, Iesus. Sancta Maria, Mater Dei,\n",
        "ora pro nobis peccatoribus, nunc et in hora mortis nostrae. Amen.\n",
        "\"\"\"\n",
        "\n",
        "re.findall('', prayer)  # Напиши свой паттерн\n"
      ],
      "metadata": {
        "id": "H2Ed3ZDkRYvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кажется, всё получилось. Но это только малая часть функционала регулярных выражений. Если стало интересно, то можно дополнительно позаниматься в тренажёре по ссылке https://regexlearn.com/learn/regex101"
      ],
      "metadata": {
        "id": "dl1eXN2WPiyt"
      }
    }
  ]
}