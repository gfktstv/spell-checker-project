{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Создание приложение\n",
        "Поздравляю, вы прошли 1 этап! Каждый из вас решил свою часть одной большой задачи — исправление опечаток. Несмотря на то, что по отдельности задача решена, воспользоваться кодом будет проблематично. Поэтому в этом блокноте мы создадим класс для нашего приложения.\n",
        "\n",
        "Что такое класс? Простыми словами это шаблон кода, по которому создаётся объект. Класс как правило имеет атрибуты (какие-то переменные класса) и методы (какие-то функции класса). Вгляните на код ниже:"
      ],
      "metadata": {
        "id": "AAAqYdCquiGD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mTJd-upueGv"
      },
      "outputs": [],
      "source": [
        "# Создаём класс собаки. Собака обязательно имеет имя и умеет гавкать\n",
        "class Dog:\n",
        "    # Создаём метод __init__, который автоматически запускается\n",
        "    # при создании класса. В нём можем указать аргумент класса\n",
        "    def __init__(self, name):\n",
        "        self.name = name  # Переменная класса, её могут использовать\n",
        "                          # методы через self.name\n",
        "\n",
        "    # Создаём другой метод, который будет возвращать строку\n",
        "    def say_woof(self):\n",
        "        string = f'{self.name} says woof!'  # Переменная метода, она может быть\n",
        "                                            # использована только внутри метода\n",
        "        return string\n",
        "\n",
        "\n",
        "\n",
        "dog1 = Dog('Rex')  # Иницилизируем объект класса, передавая в аргумент имя\n",
        "print(dog1.name)  # Атрибут имени\n",
        "print(dog1.say_woof())  # Метод класса say_woof()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Знакомо? Вы уже сталкивались с классами, просто не знали об этом. Python это объектно-ориентированный язык программирования и буквально всё в нём организовано в системе классов. Вот простой пример:"
      ],
      "metadata": {
        "id": "FSdcqjtC2GUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = str('Damn good coffee!')  # Иницилизируем объект класса str,\n",
        "                                   # передавая в аргумент строку\n",
        "print(string.upper())  # Вызываем метод класса, то есть какую-то функцию,\n",
        "                       # которая доступна для объектов класса\n",
        ""
      ],
      "metadata": {
        "id": "kcLH5yDK19zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Раз уж мы пишем на питоне, то давайте позаботимся об удобстве использования нашего приложения, для этого составим класс из тех функций, которые мы уже написали. Для того, чтобы сэкономить время, скелет нашего класса уже написан:"
      ],
      "metadata": {
        "id": "NGZuFplW22E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Для того, чтобы создать класс, напишем class [имя класса с большой буквы]:\n",
        "class SpellChecker:\n",
        "    # При создании объектов нашего класса всегда запускается функция иницилизации.\n",
        "    # В ней мы сможем указать аргумент нашего класса. Кстати, в каждом методе\n",
        "    # мы указываем self, чтобы обращаться к нашему же классу за функциями и\n",
        "    # переменными\n",
        "    def __init__(self, text):\n",
        "        self.text = text  # Переменная класса, к-ая будет содержать аргумент - текст\n"
      ],
      "metadata": {
        "id": "cMizGfGj2vZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запусти код, чтобы проверить, что получилось\n",
        "\n",
        "spell_checker_app = SpellChecker('Damn good coffee!')\n",
        "# Обратимся к переменной класса, чтобы узнать аргумент\n",
        "print(spell_checker_app.text)\n"
      ],
      "metadata": {
        "id": "sdr_b0884kOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, скелет класса готов! Теперь осталось добавить написанные ранее функции и сделать их методами. Вот инструкция:\n",
        "1. Внутри класса пишем название метода, указав в аргументе `self`. Например,\n",
        "```\n",
        "def say_woof(self):\n",
        "```\n",
        "2. Внутри метода класса вставляем то, что делает функция чтения массива слов. Добавьте в files google colab созданный одним/одной из вас файл words_massive.json и верните список слов с помощью `return`. Заметь, мы будем читать массив json в переменную без self, вот так: `words_massive = json.load(f)`. Переменные класса (те, которые с self) будем использовать только для исходного текста.\n",
        "3. Проверьте, всё ли работает: запустите ячейку с кодом после того, как заполните новый метод. Он должен выдать вам количество слов в списке."
      ],
      "metadata": {
        "id": "2OPY8yjm45Fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json  # Библиотека для работы с json файлами\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    # Напиши новый метод здесь\n",
        "\n"
      ],
      "metadata": {
        "id": "yE5Pio4742jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json  # Библиотека для работы с json файлами\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    # Напиши новый метод здесь\n",
        "    def get_words_massive(self):\n",
        "        # Откроем words_massive.json и с помощью json загрузим массив в переменную\n",
        "        with open('words_massive.json', 'r') as f:\n",
        "            words = json.load(f)\n",
        "\n",
        "        # Вернём переменную с помощью return\n",
        "        return words\n"
      ],
      "metadata": {
        "id": "XCky9uxf549g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запусти этот код, чтобы проверить, всё ли работает\n",
        "spell_checker_app = SpellChecker('Damn good coffee!')\n",
        "print(len(spell_checker_app.get_words_massive()))\n"
      ],
      "metadata": {
        "id": "VEywXR0155SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично! Первый метод добавлен. Продолжим инструкцию:\n",
        "4. Добавьте метод, который будет извлекать токены из текста, переданного в аргументе. Это делается также, как и с предыдущим методом, но теперь мы будем обращаться к переменной с текстом с помощью `self.text`. Посмотрите в чем разница:\n",
        "```\n",
        "# Работаем с функцией\n",
        "def get_tokens_dict(text):\n",
        "      # Иницилизируем части речи, которые мы не будем рассматривать\n",
        "      banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']\n",
        "\n",
        "      # Инициализируй doc с помощью функции nlp()\n",
        "      doc = nlp(text)\n",
        "```\n",
        "```\n",
        "# Работаем с методом\n",
        "def get_tokens_dict(self):\n",
        "      # Иницилизируем части речи, которые мы не будем рассматривать\n",
        "      banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']\n",
        "\n",
        "      # Инициализируй doc с помощью функции nlp()\n",
        "      doc = nlp(self.text)\n",
        "```\n",
        "\n",
        "Заполните ячейку с кодом ниже, скопировав ранее заполненный метод сверху:"
      ],
      "metadata": {
        "id": "W93JBBWv62QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json  # Библиотека для работы с json файлами\n",
        "import spacy  # Библиотека для работы с токенами\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Модель для работы с токенами\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    # Вставь сюда метод для создания списка слов\n",
        "\n",
        "\n",
        "    # Напиши новый метод здесь\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8F2V9Cs8kKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json  # Библиотека для работы с json файлами\n",
        "import spacy  # Библиотека для работы с токенами\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Модель для работы с токенами\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    # Вставь сюда метод для создания списка слов\n",
        "    def get_words_massive(self):\n",
        "        # Открой words_massive.json и с помощью json загрузи массив в переменную\n",
        "        with open('words_massive.json', 'r') as f:\n",
        "            words = json.load(f)\n",
        "\n",
        "        # Верни переменную с помощью return\n",
        "        return words\n",
        "\n",
        "    # Напиши новый метод здесь\n",
        "    def get_tokens_dict(self):\n",
        "        # Иницилизируем части речи, которые мы не будем рассматривать\n",
        "        banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']\n",
        "\n",
        "        # Обработай text с помощью nlp и сохрани обработанный текст в doc\n",
        "        doc = nlp(self.text)\n",
        "\n",
        "        # Иницилизируй словарь\n",
        "        tokens_dict = dict()\n",
        "\n",
        "        # Заполни словарь токенами и True/False c помощью цикла for\n",
        "        for token in doc:\n",
        "            key = token.text\n",
        "            value = token.pos_ in banned_pos\n",
        "\n",
        "        # Исключим перенос строки, потому что его часть речи неизвестна\n",
        "        # и если мы добавим её в banned_pos, то пропустим все опечатки,\n",
        "        # потому что их часть речи также неизвестна\n",
        "        if key == '\\n':\n",
        "            value = True\n",
        "\n",
        "        tokens_dict[key] = value\n",
        "\n",
        "        return tokens_dict\n"
      ],
      "metadata": {
        "id": "IoUkZ9hl8yXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим, всё ли работает\n",
        "spell_checker_app = SpellChecker('Damn good coffee!')\n",
        "print('Количество слов в списке:', len(spell_checker_app.get_words_massive()))\n",
        "print('Словарь токенов из текста:', spell_checker_app.get_tokens_dict())"
      ],
      "metadata": {
        "id": "pRbC4LU18zD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично! Половина пути пройдена. Продолжим:\n",
        "5. Действуем по аналогии и добавляем метод, который будет рассчитывать расстояние Левенштейна: (не забудь про то, что в аргументе будет self, а затем 2 слова)"
      ],
      "metadata": {
        "id": "4J5j9dKj9nXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json  # Библиотека для работы с json файлами\n",
        "import spacy  # Библиотека для работы с токенами\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Модель для работы с токенами\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    # Вставь сюда метод для создания списка слов\n",
        "\n",
        "\n",
        "    # Вставь сюда метод для создания словаря токенов из текста\n",
        "\n",
        "\n",
        "    # Напиши новый метод здесь\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Qeh7xu49ODg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json  # Библиотека для работы с json файлами\n",
        "import spacy  # Библиотека для работы с токенами\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Модель для работы с токенами\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    # Вставь сюда метод для создания списка слов\n",
        "    def get_words_massive(self):\n",
        "        # Открой words_massive.json и с помощью json загрузи массив в переменную\n",
        "        with open('words_massive.json', 'r') as f:\n",
        "            words = json.load(f)\n",
        "\n",
        "        # Верни переменную с помощью return\n",
        "        return words\n",
        "\n",
        "    # Вставь сюда метод для создания словаря токенов из текста\n",
        "    def get_tokens_dict(self):\n",
        "        # Иницилизируем части речи, которые мы не будем рассматривать\n",
        "        banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']\n",
        "\n",
        "        # Обработай text с помощью nlp и сохрани обработанный текст в doc\n",
        "        doc = nlp(self.text)\n",
        "\n",
        "        # Иницилизируй словарь\n",
        "        tokens_dict = dict()\n",
        "\n",
        "        # Заполни словарь токенами и True/False c помощью цикла for\n",
        "        for token in doc:\n",
        "            key = token.text\n",
        "            value = token.pos_ in banned_pos\n",
        "\n",
        "        # Исключим перенос строки, потому что его часть речи неизвестна\n",
        "        # и если мы добавим её в banned_pos, то пропустим все опечатки,\n",
        "        # потому что их часть речи также неизвестна\n",
        "        if key == '\\n':\n",
        "            value = True\n",
        "\n",
        "        tokens_dict[key] = value\n",
        "\n",
        "        return tokens_dict\n",
        "\n",
        "\n",
        "    # Напиши новый метод здесь\n",
        "    def levenstein(self, str_1, str_2):\n",
        "        n = len(str_1)\n",
        "        m = len(str_2)\n",
        "        current_row = []\n",
        "        for k in range(m+1):\n",
        "            current_row += [k]\n",
        "        for i in range(1, n+1):\n",
        "            previous_row = current_row\n",
        "            current_row = [i] + [0] * m\n",
        "            for j in range(1, m+1):\n",
        "                left = current_row[j-1] + 1\n",
        "                up = previous_row[j] + 1\n",
        "                left_up = previous_row[j-1]\n",
        "                if str_1[i-1] != str_2[j-1]:\n",
        "                    left_up += 1\n",
        "                current_row[j] = min(up, left, left_up)\n",
        "        return current_row[-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "KWR3IobV-Bu0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим, всё ли работает\n",
        "spell_checker_app = SpellChecker('Damn good coffee!')\n",
        "print('Количество слов в списке:', len(spell_checker_app.get_words_massive()))\n",
        "print('Словарь токенов из текста:', spell_checker_app.get_tokens_dict())\n",
        "print(\n",
        "    'Расстояние Левенштейна между словами cat и cot:',\n",
        "    spell_checker_app.levenstein('cat', 'cot')\n",
        "    )"
      ],
      "metadata": {
        "id": "TALJ71WI-gFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Последняя функция это `build_dict(text)`. По аналогии добавьте её в качестве метода. Не забудьте, что все обращения к функциям теперь делаются через `self`, вот так: `self.get_tokens_dict()`."
      ],
      "metadata": {
        "id": "xaxmtk38aig1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json  # Библиотека для работы с json файлами\n",
        "import spacy  # Библиотека для работы с токенами\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Модель для работы с токенами\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    # Вставь сюда метод для создания списка слов\n",
        "\n",
        "\n",
        "    # Вставь сюда метод для создания словаря токенов из текста\n",
        "\n",
        "\n",
        "    # Вставь сюда метод для рассчёта расстояния Левенштейна\n",
        "\n",
        "\n",
        "    # Напиши новый метод здесь\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UVL6UttGkedD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json  # Библиотека для работы с json файлами\n",
        "import spacy  # Библиотека для работы с токенами\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Модель для работы с токенами\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    # Вставь сюда метод для создания списка слов\n",
        "    def get_words_massive(self):\n",
        "        # Открой words_massive.json и с помощью json загрузи массив в переменную\n",
        "        with open('words_massive.json', 'r') as f:\n",
        "            words = json.load(f)\n",
        "\n",
        "        # Верни переменную с помощью return\n",
        "        return words\n",
        "\n",
        "    # Вставь сюда метод для создания словаря токенов из текста\n",
        "    def get_tokens_dict(self):\n",
        "        # Инициализируем части речи, которые мы не будем рассматривать\n",
        "        banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']\n",
        "\n",
        "        # Инициализируй словарь\n",
        "        tokens_dict = dict()\n",
        "\n",
        "        # Инициализируй doc, обработав текст с помощью функции nlp()\n",
        "        doc = nlp(self.text)\n",
        "\n",
        "        # Заполни словарь токенами и True/False c помощью цикла for\n",
        "        for token in doc:\n",
        "            key = token.text\n",
        "            value = token.pos_ in banned_pos\n",
        "\n",
        "            # Исключим перенос строки, потому что его часть речи неизвестна\n",
        "            # и если мы добавим её в banned_pos, то пропустим некоторые опечатки,\n",
        "            # потому что их часть речи также бывает неизвестна\n",
        "            if key == '\\n':\n",
        "                value = True\n",
        "\n",
        "            # Создай пару ключ-значение\n",
        "            tokens_dict[key] = value\n",
        "\n",
        "        return tokens_dict\n",
        "\n",
        "\n",
        "    # Вставь сюда метод для рассчёта расстояния Левенштейна\n",
        "    def levenstein(self, str_1, str_2):\n",
        "        n = len(str_1)\n",
        "        m = len(str_2)\n",
        "        current_row = []\n",
        "        for k in range(m+1):\n",
        "            current_row += [k]\n",
        "        for i in range(1, n+1):\n",
        "            previous_row = current_row\n",
        "            current_row = [i] + [0] * m\n",
        "            for j in range(1, m+1):\n",
        "                left = current_row[j-1] + 1\n",
        "                up = previous_row[j] + 1\n",
        "                left_up = previous_row[j-1]\n",
        "                if str_1[i-1] != str_2[j-1]:\n",
        "                    left_up += 1\n",
        "                current_row[j] = min(up, left, left_up)\n",
        "        return current_row[-1]\n",
        "\n",
        "\n",
        "    # Напиши сюда новый метод\n",
        "    def build_dict(self):\n",
        "        words_massive = self.get_words_massive()  # Получим массив образцовых слов\n",
        "        text_tokens_dict = self.get_tokens_dict()  # Получим словарь токенов\n",
        "\n",
        "        # Иницилизируем словарь, который заполним далее\n",
        "        dict_of_corrections = dict()\n",
        "        for key, value in text_tokens_dict.items():\n",
        "            dict_of_corrections[key] = {\n",
        "                'special': bool(), 'corrected': bool(), 'corrected_word': str()\n",
        "            }\n",
        "\n",
        "        # Напиши код, который заполнит словарь dict_of_corrections\n",
        "        for key, value in text_tokens_dict.items():\n",
        "            if value == True:\n",
        "                dict_of_corrections[key]['special'] = True\n",
        "                dict_of_corrections[key]['corrected'] = False\n",
        "                dict_of_corrections[key]['corrected_word'] = key\n",
        "            else:\n",
        "                if key.lower() in words_massive:\n",
        "                    dict_of_corrections[key]['special'] = False\n",
        "                    dict_of_corrections[key]['corrected'] = False\n",
        "                    dict_of_corrections[key]['corrected_word'] = key\n",
        "                else:\n",
        "                    distance = self.levenstein(key, words_massive[0])\n",
        "                    correct_word = words_massive[0]\n",
        "                for word in words_massive:\n",
        "                    dist1 = self.levenstein(key, word)\n",
        "                    if dist1 < distance:\n",
        "                        distance = dist1\n",
        "                        correct_word = word\n",
        "\n",
        "                    if distance == 1:\n",
        "                        break\n",
        "\n",
        "                dict_of_corrections[key]['special'] = False\n",
        "                dict_of_corrections[key]['corrected'] = True\n",
        "                dict_of_corrections[key]['corrected_word'] = correct_word\n",
        "\n",
        "        return dict_of_corrections\n"
      ],
      "metadata": {
        "id": "p6uOkMF1bDbU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запусти эту ячейку, чтобы проверить, всё ли работает\n",
        "\n",
        "text = \"\"\"\n",
        "Twin Peaks is an American mystery sirial drama televizion series\n",
        "created by Mark Frost and David Lynch. It premeired on ABC on April 8, 1990,\n",
        "and ran for twoo seasons until its cancellation in 1991. The show returned\n",
        "in 2017 for a third season on Showtime.\n",
        "\"\"\"\n",
        "\n",
        "spell_checker_app = SpellChecker(text)\n",
        "print('Количество слов в списке:', len(spell_checker_app.get_words_massive()))\n",
        "print('Словарь токенов из текста:', spell_checker_app.get_tokens_dict())\n",
        "print(\n",
        "    'Расстояние Левенштейна между словами cat и cot:',\n",
        "    spell_checker_app.levenstein('cat', 'cot')\n",
        "    )\n",
        "print('Словарь с исправлениями: ', spell_checker_app.build_dict())"
      ],
      "metadata": {
        "id": "nl0tl07rbtmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, класс полностью готов к использованию! Теперь скопируйте класс и вставьте в файл spell_checker.py, чтобы потом получать к нему доступ из других файлов."
      ],
      "metadata": {
        "id": "o2j4Y3J5cCT-"
      }
    }
  ]
}