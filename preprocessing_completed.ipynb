{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Первичная обработка текста: токенизация\n",
        "\n",
        "Прежде, чем находить и исправлять опечатки и ошибки, необходимо разбить текст на слова, знаки препинания и прочие языковые единицы. Этот процесс сегментации текста называется токенизации, а полученные единицы (сегменты) это токены.\n",
        "\n",
        "С первого взгляда может показаться, что достаточно просто разбить текст по пробелам и мы получим все токены. Но останутся знаки препинания. Когда и они будут учтены — окажется, например, что точка служит не только как конец предложения, но и для написания дат (21.06.2024). Кроме того, есть ещё имена собственные (New York), аббревиатуры (НГУ, ЛШ), составные слова (кресло-кровать), неразрывные неизменяемые словосочетания (и_так_далее; таким_образом), интернет-адреса (http://yandex.ru) и т. д. Короче говоря, задача на самом деле не тривиальная, но мы постараемся её решить."
      ],
      "metadata": {
        "id": "ILAZtItj8DPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Способы токенизации\n",
        "Мы рассмотрим 3 способа: составим простой алгоритм на python, воспользуемся библиотекой и попробуем использовать регулярные выражения\n",
        "### Простой алгоритм на python\n",
        "Давайте начнём с простого, разобём текст по пробелам. Для этого на python существует метод split(), который разбивает строку (str) по выбранным символам. Рассмотрим пример:"
      ],
      "metadata": {
        "id": "8LTJWkHv9pkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Все люди как пипл, а мы ФИПЛ'\n",
        "sentence.split()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zox41XaB8B4q",
        "outputId": "d464bbbf-180b-472d-ff74-5877dd135d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Все', 'люди', 'как', 'пипл,', 'а', 'мы', 'ФИПЛ']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Несмотря на то, что аргумент метода split() пустой, о умолчанию он разбивает строку на пробелы. Если сделать то же самое, но указать в качестве аргумента пробел, мы получим тот же результат:"
      ],
      "metadata": {
        "id": "iVPDYO3P-e6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.split(' ')  # В кавычках пробел\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-0wmG75-lO4",
        "outputId": "76e038f4-4c8e-4968-a959-0108c800e01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Все', 'люди', 'как', 'пипл,', 'а', 'мы', 'ФИПЛ']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод split() возвращает список (list) с элементами строки. Несложно заметить, что слово \"пипл,\" содержит запятую. Чтобы избавиться от неё, необходимо применить split(',') к каждому элементу списка. Для этого воспользуемся циклом for, который будет \"пробегать\" по каждому элементу списка и что-то с ним делать. Рассмотрим на примере:"
      ],
      "metadata": {
        "id": "TRXjtOPv-rhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for element in sentence.split(' '):  # для каждого element в списке\n",
        "  print(element)  # выводим на экран сам element\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRPYFEO1CM3z",
        "outputId": "50df1c0a-4f61-425d-e249-d6e5e992e69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Все\n",
            "люди\n",
            "как\n",
            "пипл,\n",
            "а\n",
            "мы\n",
            "ФИПЛ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Воспользуйся полученными знаниями и добавь внутрь цикла метод split(), указав в качестве аргумента запятую:"
      ],
      "metadata": {
        "id": "sj-yzoavC3o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for element in sentence.split(' '):\n",
        "  # Напиши внутри print метод split() для element\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "eCd6MDogDB8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for element in sentence.split(' '):\n",
        "  # Напиши внутри print метод split() для element\n",
        "  print(element.split(','))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76Zo8ptTKwtK",
        "outputId": "63941edf-e870-4bbd-d289-71ca7b6d7fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Все']\n",
            "['люди']\n",
            "['как']\n",
            "['пипл', '']\n",
            "['а']\n",
            "['мы']\n",
            "['ФИПЛ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Надеюсь, что у тебя всё получилось! Если бы мы учитывали все тонкости, то у нас ушло бы на это очень много времени. Поэтому рассмотрим другие решения нашей задачи."
      ],
      "metadata": {
        "id": "Y3FOk8PyDaif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Профессиональное решение\n",
        "Воспользуемся библиотекой spaCy, для этого её нужно импортировать, воспользуемся кодом ниже:"
      ],
      "metadata": {
        "id": "7QsfwKMhSaUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "LbqpCb8ITBG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google colab это популярная платформа для работы с нейросетями, поэтому нам не придётся устанавливать библиотеку. И да, spaCy использует нейросеть для токенизации и другой обработки текста. Чтобы воспользоваться spaCy нужно загрузить малую обученную модель, не будем углубляться в детали и просто воспользуемся кодом ниже:"
      ],
      "metadata": {
        "id": "UuQUJBktTF_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "I36RphIeUaj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь мы просто используем nlp как функцию для обработки строки"
      ],
      "metadata": {
        "id": "iXoxvsE6Udtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description = \"\"\"\n",
        "In computing, a pipeline, also known as a data pipeline,\n",
        "is a set of data processing elements connected in series,\n",
        "where the output of one element is the input of the next one.\n",
        "The elements of a pipeline are often executed in parallel\n",
        "or in time-sliced fashion.\n",
        "— Wikipedia\n",
        "\"\"\"\n",
        "\n",
        "nlp(description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvBlhx8qUnav",
        "outputId": "5f5fbb9f-f414-4f73-ad6e-e54873671add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "In computing, a pipeline, also known as a data pipeline, \n",
              "is a set of data processing elements connected in series, \n",
              "where the output of one element is the input of the next one. \n",
              "The elements of a pipeline are often executed in parallel \n",
              "or in time-sliced fashion. \n",
              "— Wikipedia"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выглядит не так, как раньше, ведь это не привычный нам список (list). Давайте преобразуем его с помощью функции list()"
      ],
      "metadata": {
        "id": "yb03L9p-VJ54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(nlp(description))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4re9VTsVQwK",
        "outputId": "48b9fbde-fe64-4996-b853-260fa45f7558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[,\n",
              " In,\n",
              " computing,\n",
              " ,,\n",
              " a,\n",
              " pipeline,\n",
              " ,,\n",
              " also,\n",
              " known,\n",
              " as,\n",
              " a,\n",
              " data,\n",
              " pipeline,\n",
              " ,,\n",
              " ,\n",
              " is,\n",
              " a,\n",
              " set,\n",
              " of,\n",
              " data,\n",
              " processing,\n",
              " elements,\n",
              " connected,\n",
              " in,\n",
              " series,\n",
              " ,,\n",
              " ,\n",
              " where,\n",
              " the,\n",
              " output,\n",
              " of,\n",
              " one,\n",
              " element,\n",
              " is,\n",
              " the,\n",
              " input,\n",
              " of,\n",
              " the,\n",
              " next,\n",
              " one,\n",
              " .,\n",
              " ,\n",
              " The,\n",
              " elements,\n",
              " of,\n",
              " a,\n",
              " pipeline,\n",
              " are,\n",
              " often,\n",
              " executed,\n",
              " in,\n",
              " parallel,\n",
              " ,\n",
              " or,\n",
              " in,\n",
              " time,\n",
              " -,\n",
              " sliced,\n",
              " fashion,\n",
              " .,\n",
              " ,\n",
              " —,\n",
              " Wikipedia,\n",
              " ]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот это уже похоже на правду! Давайте ближе к делу. Мы хотим получить из текста только слова, которые легко распознать как слова с опечаткой. То есть мы пометим все знаки препинания, имена собственные и так далее, чтобы потом мы могли их отсеять.\n",
        "\n",
        "Для этого мы воспользуемся словарём. Словарь это набор пар ключ-значени. Рассмотрим на примере:"
      ],
      "metadata": {
        "id": "z6UoD_KzVUp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ages_dict = {\n",
        "    'Haisenber': '52',\n",
        "    'Jesse': '23',\n",
        "}\n",
        "\n",
        "ages_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NipcGOGfVUaZ",
        "outputId": "81d87707-f85f-4585-d6e4-931a24248de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Haisenber': '52', 'Jesse': '23'}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве ключа нашего словаря будет токен, а в качестве значения True или False. Если слово НЕ подходит для рассмотрения на предмет опечаток, то True, иначе False. Рассмотрим на примере:"
      ],
      "metadata": {
        "id": "jhZG054UYWcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_dict = {\n",
        "    'Walter White': True,  # Имя собственное не рассматриваем\n",
        "    'cat': False,\n",
        "    'NSU': True,  # Аббревиатуры не рассматриваем\n",
        "    'bread': False\n",
        "}\n",
        "\n",
        "tokens_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxavmES3YnaZ",
        "outputId": "a7587c24-f024-4649-a617-e89490cbf942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Walter White': False, 'cat': True, 'NSU': False, 'bread': True}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь нам нужно сделать словарь из тех токенов, которые мы получили с помощью spaCy. Здесь важно кое-что прояснить: токены spaCy это не просто текст, но также и другая лингвистическая информация, например часть речи. Рассмотрим на примере из документации spaCy:"
      ],
      "metadata": {
        "id": "BSibz_aajiF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos, token.pos_)"
      ],
      "metadata": {
        "id": "sASgBON-ZEDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ed16fe-21d8-4e28-b4f2-f3ac0c77f9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple 96 PROPN\n",
            "is 87 AUX\n",
            "looking 100 VERB\n",
            "at 85 ADP\n",
            "buying 100 VERB\n",
            "U.K. 96 PROPN\n",
            "startup 92 NOUN\n",
            "for 85 ADP\n",
            "$ 99 SYM\n",
            "1 93 NUM\n",
            "billion 93 NUM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "У токена есть разные атрибуты, то есть переменные, которые принадлежат этому объекту. Например, атрибут `text` это токен в привычном нам формате str. Атрибуты `pos` и `pos_` это часть речи (Part Of Speech, pos) в разной записи — в формате числа и строки.\n",
        "\n",
        "Таких атрибутов у токена довольно много, но нас будут интересовать только текст и часть речи: текст будем использовать в качестве ключа словаря, а значение (True/False) определим на основе части речи. То есть какие-то части речи мы будем рассматривать для исправления опечаток, а какие-то нет. Рассмотрим на примере:"
      ],
      "metadata": {
        "id": "U9Fq2KizkYFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    key = token.text  # Текст токена будет ключем в словаре\n",
        "    value = token.pos_ != 'VERB'  # Если часть речи токена это VERB (глагол),\n",
        "                                  # То тогда значение в словаре False,\n",
        "                                  # иначе True\n",
        "\n",
        "    print(key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF1sPslNkWHs",
        "outputId": "0b2bbbd5-d4dd-4ac8-91de-1971ae029962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple False\n",
            "is False\n",
            "looking True\n",
            "at False\n",
            "buying True\n",
            "U.K. False\n",
            "startup False\n",
            "for False\n",
            "$ False\n",
            "1 False\n",
            "billion False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном примере мы используем цикл, в котором пробегаемся по всем токенам текста. В качестве ключа мы используем текст токена\n",
        "\n",
        "```key = token.text   # Переменная key содержит текст токена```\n",
        "\n",
        "В качестве значения мы хотим получить True/False в зависимости от части речи:\n",
        "\n",
        "```value = token.pos_ != 'VERB'```\n",
        "\n",
        "Символ `!=` проверяет неравенство значений слева и справа от знака. Если часть речи токена это VERB, то мы получим False и переменная value будет содержать False, в ином случае True."
      ],
      "metadata": {
        "id": "tgTjBhxIlv6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот мы и составили пары ключ-значение на основе текста. Теперь осталось отсеять все ненужные нам части речи. Мы уже подготовили список не подходящих частей речи, запусти код ниже, чтобы иницилизоровать переменную banned_pos"
      ],
      "metadata": {
        "id": "tTLobNZ61Wpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']"
      ],
      "metadata": {
        "id": "C3w-0LGHluba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы проверить, содержится ли часть речи в списке banned_pos, нужно написать следующее выражение:"
      ],
      "metadata": {
        "id": "WfHJlA81mkTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'VERB' in banned_pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS0Xmep115WB",
        "outputId": "e005c52e-5794-47d9-e2ff-248dfd2fa0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь давайте решать нашу задачу. Для начала напиши код, который будет выводить на экран слово и False/True в зависимости от того, находится ли оно в banned_pos"
      ],
      "metadata": {
        "id": "cuZvsbFY3kgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Jesse, we have to cook'\n",
        "\n",
        "# Напиши свой код ниже\n"
      ],
      "metadata": {
        "id": "Q8cIDf0X3j3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Jesse, we have to cook'\n",
        "\n",
        "# Напиши свой код ниже\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "  key = token.text\n",
        "  value = token.pos_ in banned_pos\n",
        "  print(key, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZKyLQL2L7qF",
        "outputId": "5dcaa316-6e0c-4b14-dc0b-044f2bb90d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jesse True\n",
            ", True\n",
            "we False\n",
            "have False\n",
            "to True\n",
            "cook False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, теперь сделаем из этого словарь. Просто добавь перед циклом for переменную и иницилизируй словарь вот так:\n",
        "\n",
        "```tokens_dict = dict()```\n",
        "\n",
        "Затем вместо того, чтобы напечатать слово и значение True/False сделаем как пару ключ/значение\n",
        "\n",
        "```\n",
        "for token in doc:\n",
        "  key =  # здесь текст токена\n",
        "  value =  # здесь True, если часть речи в banned_pos\n",
        "           # и False, если часть речи НЕ в banned_pos\n",
        "  \n",
        "  tokens_dict[key] = value  # создаём пару ключ-значение\n",
        "```\n",
        "\n",
        "Напиши код ниже и выведи словарь на экран с помощью print(). Заметь, что в цикле появилась дополнительная проверка с комментарием \"Исключим перенос строки\". Не удаляй её и вставь в последующие циклы."
      ],
      "metadata": {
        "id": "3X0sHLvx50R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Jesse, we have to cook'\n",
        "\n",
        "# Иницилизируем части речи, которые мы не будем рассматривать\n",
        "banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']\n",
        "\n",
        "# Иницилизируй словарь перед циклом\n",
        "\n",
        "\n",
        "for token in doc:\n",
        "  key =  # Присвой переменной key значение текста токена\n",
        "  value =   # Присвой переменной value True/False в зависимости от части речи\n",
        "\n",
        "  # Исключим перенос строки, потому что его часть речи неизвестна\n",
        "  # и если мы добавим её в banned_pos, то пропустим все опечатки,\n",
        "  # потому что их часть речи также неизвестна\n",
        "  if key == '\\n':\n",
        "    value = True\n",
        "\n",
        "  # Создай пару ключ-значение\n",
        "\n",
        "\n",
        "print()  # Напиши переменную словаря внутрь функции print\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "8_oK3OaW5xBh",
        "outputId": "cb9afa05-487d-4e04-a00f-5b8775b12fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-75-74dee08bf0e2>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-75-74dee08bf0e2>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    key =  # Присвой переменной key значение текста токена\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Jesse, we have to cook'\n",
        "\n",
        "# Иницилизируем части речи, которые мы не будем рассматривать\n",
        "banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']\n",
        "\n",
        "# Иницилизируй словарь перед циклом\n",
        "tokens_dict = dict()\n",
        "\n",
        "for token in doc:\n",
        "  key = token.text\n",
        "  value = token.pos_ in banned_pos\n",
        "\n",
        "  # Исключим перенос строки, потому что его часть речи неизвестна\n",
        "  # и если мы добавим её в banned_pos, то пропустим все опечатки,\n",
        "  # потому что их часть речи также неизвестна\n",
        "  if key == '\\n':\n",
        "    value = True\n",
        "\n",
        "  # Создай пару ключ-значение\n",
        "  tokens_dict[key] = value\n",
        "\n",
        "print(tokens_dict)  # Напиши переменную словаря внутрь функции print\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwHAoKrOMR4x",
        "outputId": "f80e5165-7c66-4dc3-c1a2-90506df2112d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Jesse': True, ',': True, 'we': False, 'have': False, 'to': True, 'cook': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, теперь осталось оформить всё это в функции. Функция работает очень просто: мы даём ей какое-то название, определяем, будет ли она что-то принимать в качестве аргумента и затем пишем внутри то, что она будет делать. Вот так:"
      ],
      "metadata": {
        "id": "Yk-oVEMo2IUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tokens(text):  # Def это функция, print_string это название,\n",
        "                           # String это аргумент\n",
        "  print(text.split())\n",
        "\n",
        "text = 'Jesse, we have to cook'  # Напишем какую-нибудь строку\n",
        "print_tokens(text)  # Вызовем функцию и передадим ей эту строку"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVgED_LD2O9h",
        "outputId": "a1911f37-f726-450e-e1b8-16131a3a7946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jesse,', 'we', 'have', 'to', 'cook']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь финальное задание. У нас есть функция get_tokens_dictionary, её аргумент это какая-то строка. Нам нужно взять тот код, который на основе строки создаёт словарь и вставить его внутрь функции.\n",
        "\n",
        "И самое главное, создание словаря и его заполнение должно быть внутри функции."
      ],
      "metadata": {
        "id": "vKso4LZh3Rv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens_dict(text):\n",
        "  # Иницилизируем части речи, которые мы не будем рассматривать\n",
        "  banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']\n",
        "\n",
        "  # Обработай text с помощью nlp и сохрани обработанный текст в doc\n",
        "\n",
        "\n",
        "  # Иницилизируй словарь\n",
        "\n",
        "\n",
        "  # Заполни словарь токенами и True/False c помощью цикла for\n",
        "\n",
        "\n",
        "  return tokens_dict"
      ],
      "metadata": {
        "id": "V8OC30ji3Q9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens_dict(text):\n",
        "  # Иницилизируем части речи, которые мы не будем рассматривать\n",
        "  banned_pos = ['PROPN', 'SYM', 'PART', 'CCONJ', 'ADP', 'PUNCT']\n",
        "\n",
        "  # Обработай text с помощью nlp и сохрани обработанный текст в doc\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Иницилизируй словарь\n",
        "  tokens_dict = dict()\n",
        "\n",
        "  # Заполни словарь токенами и True/False c помощью цикла for\n",
        "  for token in doc:\n",
        "    key = token.text\n",
        "    value = token.pos_ in banned_pos\n",
        "\n",
        "    # Исключим перенос строки, потому что его часть речи неизвестна\n",
        "    # и если мы добавим её в banned_pos, то пропустим все опечатки,\n",
        "    # потому что их часть речи также неизвестна\n",
        "    if key == '\\n':\n",
        "      value = True\n",
        "\n",
        "    tokens_dict[key] = value\n",
        "\n",
        "  return tokens_dict"
      ],
      "metadata": {
        "id": "wqf8Ea0hNHuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запусти этот код, чтобы проверить, работает ли функция\n",
        "\n",
        "sample_text = '''\n",
        "Levenshtein distnce, also knowm as edit distance, is a metric\n",
        "fo measuring the difference between two sequencess.\n",
        "It represents the minimum numbet of single-character editk\n",
        "(insertions, deletons, or substititions) requirуd to change\n",
        "one word or string ingo the other.\n",
        "'''\n",
        "\n",
        "get_tokens_dict(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfmjecu8-EWp",
        "outputId": "26f74e91-2ad1-4b63-f6cd-e4d1a7342448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': True,\n",
              " 'Levenshtein': True,\n",
              " 'distnce': False,\n",
              " ',': True,\n",
              " 'also': False,\n",
              " 'knowm': False,\n",
              " 'as': True,\n",
              " 'edit': False,\n",
              " 'distance': False,\n",
              " 'is': False,\n",
              " 'a': False,\n",
              " 'metric': False,\n",
              " 'fo': True,\n",
              " 'measuring': False,\n",
              " 'the': False,\n",
              " 'difference': False,\n",
              " 'between': True,\n",
              " 'two': False,\n",
              " 'sequencess': False,\n",
              " '.': True,\n",
              " 'It': False,\n",
              " 'represents': False,\n",
              " 'minimum': False,\n",
              " 'numbet': False,\n",
              " 'of': True,\n",
              " 'single': False,\n",
              " '-': True,\n",
              " 'character': False,\n",
              " 'editk': False,\n",
              " '(': True,\n",
              " 'insertions': False,\n",
              " 'deletons': False,\n",
              " 'or': True,\n",
              " 'substititions': False,\n",
              " ')': True,\n",
              " 'requirуd': False,\n",
              " 'to': True,\n",
              " 'change': False,\n",
              " 'one': False,\n",
              " 'word': False,\n",
              " 'string': False,\n",
              " 'ingo': True,\n",
              " 'other': False}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Надеюсь, что всё работает! Далее мы будем использовать эту функцию в приложении, поэтому не сохрани свой результат (ctrl + s)"
      ],
      "metadata": {
        "id": "OA8wOSWdPvnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Алтернативный способ: регулярные выражения\n",
        "Задача решена. Но какие есть другие способы токенизировать текст? Одним из неплохих решений можно считать использование регулярных выражений, давайте посмотрим что это такое"
      ],
      "metadata": {
        "id": "sfWjFTswDsL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # Импортируем библиотеку регулярных выражений\n",
        "\n",
        "sentence = 'Все люди, как пипл, а мы ФИПЛ'\n",
        "print(re.split(' |, ', sentence), 'regex в деле')\n",
        "print(sentence.split(' |, '), 'встроенный split')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0x_9Mt6Ev9k",
        "outputId": "086bdeed-4c26-4226-9f92-64f77c23f79f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Все', 'люди', 'как', 'пипл', 'а', 'мы', 'ФИПЛ'] regex в деле\n",
            "['Все люди, как пипл, а мы ФИПЛ'] встроенный split\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как же это работает? Регулярные выражения позволяют использовать в качестве паттерна различные символы. Кажется, стало только непонятней. Рассмотрим на примере:\n",
        "\n",
        "`re.split(' |, ', sentence)`\n",
        "\n",
        "re.split использует в качестве аргумента для разбиения строки паттерн в кавычках `' |, '`, в котором указан символ `|`, он же \"или\". Этот символ позволяет выбирать по каким символам разбивать строку: по пробелу (указан слева от `|`) или по запятой и пробелу (указаны справа от `|`). Поэтому в итоге слово \"пипл\" не содержит запятой, ведь после него стояла запятая и пробел, которую учёл regex."
      ],
      "metadata": {
        "id": "u0qUzaLhFm8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Но что, если мы хотим учесть точку и другие знаки препинания? Для этого воспользуемся в паттерне квадратными скобками: они позволяют нам учесть все паттерны, в которых содержится один из элементов, указанных внутри квадратных скобок. Рассмотрим на примере:"
      ],
      "metadata": {
        "id": "SC2ItcWyHMMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Все люди, как пипл, а мы фипл'\n",
        "re.findall('[фп]ипл', sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvJCpTUzHLkH",
        "outputId": "2af5f4d8-ce1f-494b-e539-bba6d82d152c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['пипл', 'фипл']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод findall позволяет найти всё то, что указано в паттерне. Как видно, в паттерне мы указали `'[фп]ипл'`, то есть мы ищем либо фипл (ф из квадратных скобок), либо пипл (п из квадратных скобок).\n",
        "\n",
        "Теперь попробуй самостоятельно дополнить паттерн `' |, '` другими знаками препинания, используя квадратные скобки на месте запятой"
      ],
      "metadata": {
        "id": "aj_sU1SpIZa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Все люди, как пипл, а мы ФИПЛ'\n",
        "re.split(' |, ', sentence)"
      ],
      "metadata": {
        "id": "CFoKii_KHx2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь, когда мы знаем что такое регулярные выражение, что такое паттерн и какие символы он может использовать — рассмотрим как с помощью него можно осуществить токенизацию текста:"
      ],
      "metadata": {
        "id": "5PwYQcYfI7g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prayer = \"\"\"\n",
        "Ave, Maria, gratia plena; Dominus tecum; benedicta tu in mulieribus,\n",
        "et benedictus fructus ventris tui, Iesus. Sancta Maria, Mater Dei,\n",
        "ora pro nobis peccatoribus, nunc et in hora mortis nostrae. Amen.\n",
        "\"\"\"\n",
        "\n",
        "re.findall('\\w+', prayer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6pLsBpzJGzX",
        "outputId": "44627c2c-a17a-47f8-be6c-abddd08298f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ave',\n",
              " 'Maria',\n",
              " 'gratia',\n",
              " 'plena',\n",
              " 'Dominus',\n",
              " 'tecum',\n",
              " 'benedicta',\n",
              " 'tu',\n",
              " 'in',\n",
              " 'mulieribus',\n",
              " 'et',\n",
              " 'benedictus',\n",
              " 'fructus',\n",
              " 'ventris',\n",
              " 'tui',\n",
              " 'Iesus',\n",
              " 'Sancta',\n",
              " 'Maria',\n",
              " 'Mater',\n",
              " 'Dei',\n",
              " 'ora',\n",
              " 'pro',\n",
              " 'nobis',\n",
              " 'peccatoribus',\n",
              " 'nunc',\n",
              " 'et',\n",
              " 'in',\n",
              " 'hora',\n",
              " 'mortis',\n",
              " 'nostrae',\n",
              " 'Amen']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью findall мы ищем всё, что подходит под паттерн `\\w+`, который состоит из двух элементов: `\\w` и `+`. `\\w` идентичен `'[a-zA-Z0-9_]'`, а `+` это 1 или более символов. То есть мы ищем 1 или более символов из набора всех латинских букв (строчных и прописных), цифр и нижнего подчёркивания.\n",
        "\n",
        "Проверь это, записав re.findall с использованием идентичного паттерна вместо `\\w`"
      ],
      "metadata": {
        "id": "jhbT6vIMOM1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prayer = \"\"\"\n",
        "Ave, Maria, gratia plena; Dominus tecum; benedicta tu in mulieribus,\n",
        "et benedictus fructus ventris tui, Iesus. Sancta Maria, Mater Dei,\n",
        "ora pro nobis peccatoribus, nunc et in hora mortis nostrae. Amen.\n",
        "\"\"\"\n",
        "\n",
        "re.findall('', prayer)  # Напиши свой паттерн"
      ],
      "metadata": {
        "id": "H2Ed3ZDkRYvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prayer = \"\"\"\n",
        "Ave, Maria, gratia plena; Dominus tecum; benedicta tu in mulieribus,\n",
        "et benedictus fructus ventris tui, Iesus. Sancta Maria, Mater Dei,\n",
        "ora pro nobis peccatoribus, nunc et in hora mortis nostrae. Amen.\n",
        "\"\"\"\n",
        "\n",
        "re.findall('[a-zA-Z0-9_]+', prayer)  # Напиши свой паттерн"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItHIooRJRTO7",
        "outputId": "08e26eee-4695-4b3a-a8b2-cf080e6bd1eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ave',\n",
              " 'Maria',\n",
              " 'gratia',\n",
              " 'plena',\n",
              " 'Dominus',\n",
              " 'tecum',\n",
              " 'benedicta',\n",
              " 'tu',\n",
              " 'in',\n",
              " 'mulieribus',\n",
              " 'et',\n",
              " 'benedictus',\n",
              " 'fructus',\n",
              " 'ventris',\n",
              " 'tui',\n",
              " 'Iesus',\n",
              " 'Sancta',\n",
              " 'Maria',\n",
              " 'Mater',\n",
              " 'Dei',\n",
              " 'ora',\n",
              " 'pro',\n",
              " 'nobis',\n",
              " 'peccatoribus',\n",
              " 'nunc',\n",
              " 'et',\n",
              " 'in',\n",
              " 'hora',\n",
              " 'mortis',\n",
              " 'nostrae',\n",
              " 'Amen']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "И это только малая часть функционала регулярных выражений. Дома можете поиграться с регулярными выражениями в тренажёре по ссылке https://regexlearn.com/learn/regex101"
      ],
      "metadata": {
        "id": "dl1eXN2WPiyt"
      }
    }
  ]
}